summarise(across(everything(), mean))
data
# We add cluster info to original data. We use the mean model, as we want to understand the
# relationships between the original variables and the clusters)
data <- data[, mean_vars]
data$cluster <- as.factor(clusters_mean)
# We compute mean per cluster
data %>%
group_by(cluster) %>%
summarise(across(everything(), mean))
# Add cluster info to original data
clusters <- km.out_1$cluster
USArrests$cluster <- as.factor(clusters)
# Compute mean per cluster
USArrests %>%
group_by(cluster) %>%
summarise(across(everything(), mean))
## Clustering ##
# INTRODUCTION #
library(FactoMineR)
# We will analyse a dataset about Crime Rates in USA cities
data("USArrests")
head(USArrests)
# Our goal is to group the states
# (whose coordinates are given by the PCA) with Clustering methods
# PCA #
# Compute the PCA
pca_res <- PCA(USArrests, scale.unit=TRUE)
pca_data <- pca_res$ind$coord[,1:2]
# Interpretation
# Have a look at the correlation diagram.
# The first component (0.62) measures the rate of crime in a state
# The second component (0.25) measures the population of urban areas
# K-means #
k <- 4
km.out_1 <- kmeans(pca_data,
k,
nstart = 1 # how many initial conditions are created
)
km.out_2 <- kmeans(pca_data,
k,
nstart = 1 # how many initial conditions are created
)
# We can plot the data, with each observation colored
# according to its cluster assignment.
par(mfrow = c(2, 1))
plot(pca_data, col = (km.out_1$cluster + 1), ylim = c(-3,2),
main = paste("K-Means Clustering Results with K = ", k, sep = " "),
xlab = "", ylab = "", pch = 20, cex = 2)
plot(pca_data, col = (km.out_2$cluster + 1), ylim = c(-3,2),
main = paste("K-Means Clustering Results with K = ", k, sep = " "),
xlab = "", ylab = "", pch = 20, cex = 2)
# what do you notice in the 2 plots?
# what happens if we set the seed?
set.seed(0)
# what does this imply when we run K-means?
k <- 4
km.out_1 <- kmeans(pca_data,
k,
nstart = 1 # how many initial conditions are created
)
km.out_2 <- kmeans(pca_data,
k,
nstart = 1 # how many initial conditions are created
)
par(mfrow = c(2, 1))
plot(pca_data, col = (km.out_1$cluster + 1), ylim = c(-3,2),
main = paste("K-Means Clustering Results with K = ", k, sep = " "),
xlab = "", ylab = "", pch = 20, cex = 2)
plot(pca_data, col = (km.out_2$cluster + 1), ylim = c(-3,2),
main = paste("K-Means Clustering Results with K = ", k, sep = " "),
xlab = "", ylab = "", pch = 20, cex = 2)
# what do you notice in the 2 plots?
# a smarter way of running K-means is to use the parameter nstart
k <- 3
km.out_1 <- kmeans(pca_data,
k,
nstart = 10 # how many initial conditions are created
)
km.out_2 <- kmeans(pca_data,
k,
nstart = 10 # how many initial conditions are created
)
# nstart parameter is an argument that specifies
# the number of initial random cluster assignments to be tried.
# It is used to find the best initial set of cluster centers
# by repeatedly running the clustering process
# with different initializations and selecting the one with
# the lowest total within-cluster sum of squares (WCSS).
#
# Note: The default value for nstart is 1, meaning the algorithm
# is run only once with a single set of initial cluster centers.
# We can increase nstart to a larger number (e.g., 10, 20, etc.)
# to improve the chances of finding a better clustering solution.
par(mfrow = c(2, 1))
plot(pca_data, col = (km.out_1$cluster + 1), ylim = c(-3,2),
main = paste("K-Means Clustering Results with K = ", k, sep = " "),
xlab = "", ylab = "", pch = 20, cex = 2)
plot(pca_data, col = (km.out_2$cluster + 1), ylim = c(-3,2),
main = paste("K-Means Clustering Results with K = ", k, sep = " "),
xlab = "", ylab = "", pch = 20, cex = 2)
# show again the observations as function of the PCs, using the clusters
# from k-means and showing the names of the states
par(mfrow = c(1, 1))
plot(pca_data, col = (km.out_1$cluster + 1), ylim = c(-3,2),
main = paste("K-Means Clustering Results with K = ", k, sep = " "),
xlab = "", ylab = "", pch = 20, cex = 2)
# Add labels to every point
text(pca_data, pos = 1, labels = row.names(USArrests), cex = .5)
# Here the observations can be easily plotted because they are two-dimensional.
# If there were more than two variables then we could instead perform PCA
# and plot the first two principal components score vectors.
## Hierarchical clustering ##
# We first have to compute a Distance Matrix, where
# we can choose among different distances, e.g. euclidean, manhattan:
distance_matrix <- dist(pca_data, method = "euclidean")
distance_matrix
# Then, we apply the hclust function choosing the method (complete/single)
hc <-  hclust(distance_matrix, method = "complete")
hc
# We can plot the dendrogram
plot(hc)
# To determine the cluster for each observation associated
# with a given cut of the dendrogram, we can use the cutree() function.
# If we want to cut horizontally:
cutree(hc, h=5)  # setting the height
# If we want to cut vertically:
cutree(hc, k=2)  # setting the number of clusters
### Elbow plot
library(factoextra)
# Elbow plot for kmeans
fviz_nbclust(pca_data, kmeans, method = "wss", k.max = 20)  +
geom_vline(xintercept = 3, linetype = 2) +
geom_vline(xintercept = 5, linetype = 2)
# Elbow plot for hierarchical clustering
fviz_nbclust(pca_data, hcut, method = "wss", k.max = 20)
# Does it make sense to consider WSS (within-cluster sum of squares (WCSS))
# in the case of Hierarchical Clustering?
# IKER: yes and no --> K-means works exactly to optimize WCSS, so the elbow plot is easier
# to interpret, this is not the case of hierarchical clustering. But still, WCSS is a
# distance measurement, so it gives information about the clusters and is OK to use.
# In case of dubious results, change the method to "silhouette"
fviz_nbclust(pca_data, hcut, method = "silhouette", k.max = 20)
# (The Silhouette score is a measure of how similar a point is
# to its own cluster compared to other clusters. The scores of all points
# are then used to compare the average silhouette score/width)
# Here some info about the Silhouette method:
# https://www.baeldung.com/cs/silhouette-values-clustering
# https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html
## Clustering interpretation ##
library(dplyr)
library(ggplot2)
library(tidyr)
# Add cluster info to original data
clusters <- km.out_1$cluster
USArrests$cluster <- as.factor(clusters)
# Compute mean per cluster
USArrests %>%
group_by(cluster) %>%
summarise(across(everything(), mean))
# We compute mean per cluster
data %>%
group_by(cluster) %>%
summarise(across(where(is.numeric), mean), .groups = "drop")
# CLUSTERING (K-MEANS) OF THE WISCONSIN BREAST CANCER DATASET
# Authors: Iker Ibarrola, Enetz Quindimil, Carlos Firvida, IÃ±igo Infante
# We load the libraries we will used to create the model:
library(FactoMineR)
library(dplyr)
library(ggplot2)
library(factoextra)
library(reshape2)
###################################
# DATASET LOADING & CHECKING
###################################
data <- read.csv("./breast_cancer_data.csv") # We read the dataset from the csv file
head(data) # We visualize the first six rows of the dataset
str(data) # We visualize the structure of the data (the different variables, their type and some initial values)
###################################
# DATASET MODIFICATION
###################################
# We remove the variables that are not continuous:
#* Before removing it we will save the diagnosis in a variable, as it will be useful
# at the end of the analysis
diagnosis <- as.factor(data$diagnosis)
data <- data %>% select(-id, -diagnosis) # We remove
# We standardize the variables to have mean zero and standard deviation one.
data_scaled <- as.data.frame(scale(data))
###################################
# APPROACH SELECTION
###################################
# In this case, we have decided to study the two approaches proposed in the guidelines:
# Apply K-means to the components selected during PCA. As we found during the PCA
# analysis, the model with just the 'mean' variables is more than enough to get
# most of the information from the data, so that is the one we have selected.
# Apply K-means directly to the original variables. In this case, we will also use
# the mean variables, leaving out the ones that have been proved to be redundant or
# not so meaningful.
# We select the variables that measure the means (this data will be used in both approaches,
# the one for PCA and the one for the original variables).
mean_vars <- grep("_mean$", names(data_scaled), value = TRUE)
data_mean <- data_scaled[, mean_vars]
data_mean <- cbind(data_mean, diagnosis)
# We also divide the benign and malign diagnosis
data_mean_benign <- data_mean %>% filter(diagnosis == "B")
data_mean_malign <- data_mean %>% filter(diagnosis == "M")
# We remove the diagnosis as it is a categorical variable
data_mean <- data_mean %>% select(-diagnosis) # We remove
data_mean_benign <- data_mean_benign %>% select(-diagnosis) # We remove
data_mean_malign <- data_mean_malign %>% select(-diagnosis) # We remove
# We compute the PCA
pca_mean <- PCA(data_mean, scale.unit = TRUE)
data_pca <- pca_mean$ind$coord[,1:2] # After checking different number of components,
# we have seen that 2 components are enough, so we keep them.
###################################
# K-MEANS APPLICATION
###################################
###################################
#### Selecting K
# First of all, we will decide the number of clusters. We know that there are two different
# groups in our data, the ones which are malign tumors and the ones that are benign.
# However, we will create an elbow plot to select the appropiate number of clusters.
# We set the seed for reproducibility
set.seed(123456)
# We create the data frames where we will store the results
result_mean = data.frame()
result_pca = data.frame()
result_mean_benign = data.frame()
result_mean_malign = data.frame()
# We run kmeans with 50 different k so that we can visualize the elbow plot and select
# an appropiate number of clusters (we do this for all the cases)
for (i in 1:50){
# For the mean 'model'
fit_mean = kmeans(data_mean, i,nstart = 10)
result_mean = rbind(result_mean,data.frame(k=i,withinss=fit_mean$tot.withinss))
# For the benign 'model'
fit_mean_benign = kmeans(data_mean_benign, i,nstart = 10)
result_mean_benign = rbind(result_mean_benign,data.frame(k=i,withinss=fit_mean_benign$tot.withinss))
# For the malign 'model'
fit_mean_malign = kmeans(data_mean_malign, i,nstart = 10)
result_mean_malign = rbind(result_mean_malign,data.frame(k=i,withinss=fit_mean_malign$tot.withinss))
# For the PCA 'model'
fit_pca = kmeans(data_pca, i,nstart = 10)
result_pca = rbind(result_pca,data.frame(k=i,withinss=fit_pca$tot.withinss))
}
# We plot the elbow plot for all the variables
ggplot(data=result_mean,aes(x=k,y=withinss))+
geom_line()+
geom_point()
# We plot the elbow plot for all the benign variables
ggplot(data=result_mean_benign,aes(x=k,y=withinss))+
geom_line()+
geom_point()
# We plot the elbow plot for all the malign variables
ggplot(data=result_mean_malign,aes(x=k,y=withinss))+
geom_line()+
geom_point()
# We plot the elbow plot for PCA
ggplot(data=result_pca,aes(x=k,y=withinss))+
geom_line()+
geom_point()
# As we have seen that the elbow plot is very similar in the cases of the mean model and
# the benign & malign ones, we will continues by keeping just the mean one
# We apply the silhouette method to all the 'models'
fviz_nbclust(data_mean, hcut, method = "silhouette", k.max = 100)
fviz_nbclust(data_pca, hcut, method = "silhouette", k.max = 100)
# The silhouette method shows a result that is not alligned with the elbow plot. In this
# case, we will keep what the elbow plot says: even though elbow plots are not completely
# clear, we believe that in both cases the number of cluster could be around 15. As we
# want to compare the two models (all variables and PCA), we will select K = 15.
###################################
#### Running K-means
# The chosen number of clusters
k <- 15
# We apply K-means to the mean model
km_mean <- kmeans(data_mean, k, nstart = 10)
clusters_mean <- km_mean$cluster
# We apply K-means to the PCA model
km_pca <- kmeans(data_pca, k, nstart = 10)
clusters_pca <- km_pca$cluster
###################################
#### Association of clusters
# We create a contingency table
cluster_comparison <- table(
Original = clusters_mean,
PCA = clusters_pca
)
# We change the format of the table to print it
cluster_df <- melt(cluster_comparison)
names(cluster_df) <- c("Original", "PCA", "Common_points")
# We create a heatmap to see how related the clusters are
heatmap_plot <- ggplot(cluster_df, aes(x = PCA, y = Original, fill = Common_points)) +
geom_tile() +
scale_fill_gradient(low = "white", high = "red") +
#
scale_x_continuous(breaks = seq(1, k, by = 1)) +
scale_y_continuous(breaks = seq(1, k, by = 1)) +
labs(
title = "Comparison of clusters: Original Variables vs PCA",
x = "Clusters based on PCA",
y = "Clusters based on original variables",
fill = "Common points"
) +
theme_minimal() +
theme(
axis.text.x = element_text(angle = 0, hjust = 0.5),
plot.title = element_text(hjust = 0.5)
) +
# We add the exact values to the tiles
geom_text(aes(label = Common_points), color = "black")
# We visualize the heatmap
print(heatmap_plot)
# In the heat map we can see how the clusters from the 'mean' model and from the PCA model
# are related. To be honest, this is not what we were expecting. We expected to see, more
# or less clearly, how the different clusters of each model where associated (something
# like "cluster x from the mean model represents clearly cluster y from the PCA model"),
# but we have got a matrix in which most points of a cluster are distributed across
# several clusters of the other model. The most clear patterns/relations we have been able
# to identify are the following:
# Cluster 15 of the mean model is most similar to cluster 8.
# Cluster 12 of the mean model is  mostly a combination of 1 and 12.
# Cluster 10 of the mean model is mostly a combination of 2 and 14.
# Cluster 8 of the mean model is most similar to cluster 3.
# Cluster 7 of the mean model is most similar to cluster 1.
# Cluster 6 of the mean model is EXACTLY cluster 5.
# Cluster 5 of the mean model is  mostly a combination of 11 and 13.
# Cluster 4 of the mean model is most similar to cluster 2.
# Cluster 3 of the mean model is EXACTLY cluster 4.
# Cluster 2 of the mean model is most similar to cluster 6.
# Cluster 1 of the mean model is  mostly a combination of 10 and 15.
# Clusters 9, 11, 13, 14 of the mean model are more spread across multiple clusters of the
# PCA model.
# This means that there is a clear correspondence between the clustering of the mean
# and the PCA models.
# As we know that PCA significantly alters the structure of the data, and taking into account
# that we are reducing the dimensionality of the data from 10 to just 2, the fact that most
# relationships can still be detected are good news, meaning we are still keeping somehow the
# original structure.
###################################
# K-MEANS VISUALIZATION & INTERPRETATION
###################################
###################################
#### K-means visualization
# We will use the rainbow function as used in the "NCI60" script to assign a color to
# each cluster
colors <- rainbow(15)
# We visualize the clusters in the case of PCA (as we can visualize it in 2D)
plot(data_pca, col = colors[km_pca$cluster],
ylim = c(-3, 2),
main = paste("K-Means Clustering Results with K =", k),
xlab = "", ylab = "", pch = 20, cex = 2)
# As we are plotting the clusters in the case of the PCA model, and we want to intrepet
# the results based on the original variables, we will recall the plot showing the
# relationship between the origninal variables and the first two Principal Components
fviz_pca_var(pca_mean)
###################################
#### K-means interpretation
# We add cluster info to original data. We use the mean model, as we want to understand the
# relationships between the original variables and the clusters)
data <- data[, mean_vars]
data$cluster <- as.factor(clusters_mean)
# We compute mean per cluster
data %>%
group_by(cluster) %>%
summarise(across(everything(), mean))
## Clustering ##
# INTRODUCTION #
library(FactoMineR)
# We will analyse a dataset about Crime Rates in USA cities
data("USArrests")
head(USArrests)
# Our goal is to group the states
# (whose coordinates are given by the PCA) with Clustering methods
# PCA #
# Compute the PCA
pca_res <- PCA(USArrests, scale.unit=TRUE)
pca_data <- pca_res$ind$coord[,1:2]
# Interpretation
# Have a look at the correlation diagram.
# The first component (0.62) measures the rate of crime in a state
# The second component (0.25) measures the population of urban areas
# K-means #
k <- 4
km.out_1 <- kmeans(pca_data,
k,
nstart = 1 # how many initial conditions are created
)
km.out_2 <- kmeans(pca_data,
k,
nstart = 1 # how many initial conditions are created
)
# We can plot the data, with each observation colored
# according to its cluster assignment.
par(mfrow = c(2, 1))
plot(pca_data, col = (km.out_1$cluster + 1), ylim = c(-3,2),
main = paste("K-Means Clustering Results with K = ", k, sep = " "),
xlab = "", ylab = "", pch = 20, cex = 2)
plot(pca_data, col = (km.out_2$cluster + 1), ylim = c(-3,2),
main = paste("K-Means Clustering Results with K = ", k, sep = " "),
xlab = "", ylab = "", pch = 20, cex = 2)
# what do you notice in the 2 plots?
# what happens if we set the seed?
set.seed(0)
# what does this imply when we run K-means?
k <- 4
km.out_1 <- kmeans(pca_data,
k,
nstart = 1 # how many initial conditions are created
)
km.out_2 <- kmeans(pca_data,
k,
nstart = 1 # how many initial conditions are created
)
par(mfrow = c(2, 1))
plot(pca_data, col = (km.out_1$cluster + 1), ylim = c(-3,2),
main = paste("K-Means Clustering Results with K = ", k, sep = " "),
xlab = "", ylab = "", pch = 20, cex = 2)
plot(pca_data, col = (km.out_2$cluster + 1), ylim = c(-3,2),
main = paste("K-Means Clustering Results with K = ", k, sep = " "),
xlab = "", ylab = "", pch = 20, cex = 2)
# what do you notice in the 2 plots?
# a smarter way of running K-means is to use the parameter nstart
k <- 3
km.out_1 <- kmeans(pca_data,
k,
nstart = 10 # how many initial conditions are created
)
km.out_2 <- kmeans(pca_data,
k,
nstart = 10 # how many initial conditions are created
)
# nstart parameter is an argument that specifies
# the number of initial random cluster assignments to be tried.
# It is used to find the best initial set of cluster centers
# by repeatedly running the clustering process
# with different initializations and selecting the one with
# the lowest total within-cluster sum of squares (WCSS).
#
# Note: The default value for nstart is 1, meaning the algorithm
# is run only once with a single set of initial cluster centers.
# We can increase nstart to a larger number (e.g., 10, 20, etc.)
# to improve the chances of finding a better clustering solution.
par(mfrow = c(2, 1))
plot(pca_data, col = (km.out_1$cluster + 1), ylim = c(-3,2),
main = paste("K-Means Clustering Results with K = ", k, sep = " "),
xlab = "", ylab = "", pch = 20, cex = 2)
plot(pca_data, col = (km.out_2$cluster + 1), ylim = c(-3,2),
main = paste("K-Means Clustering Results with K = ", k, sep = " "),
xlab = "", ylab = "", pch = 20, cex = 2)
# show again the observations as function of the PCs, using the clusters
# from k-means and showing the names of the states
par(mfrow = c(1, 1))
plot(pca_data, col = (km.out_1$cluster + 1), ylim = c(-3,2),
main = paste("K-Means Clustering Results with K = ", k, sep = " "),
xlab = "", ylab = "", pch = 20, cex = 2)
# Add labels to every point
text(pca_data, pos = 1, labels = row.names(USArrests), cex = .5)
# Here the observations can be easily plotted because they are two-dimensional.
# If there were more than two variables then we could instead perform PCA
# and plot the first two principal components score vectors.
## Hierarchical clustering ##
# We first have to compute a Distance Matrix, where
# we can choose among different distances, e.g. euclidean, manhattan:
distance_matrix <- dist(pca_data, method = "euclidean")
distance_matrix
# Then, we apply the hclust function choosing the method (complete/single)
hc <-  hclust(distance_matrix, method = "complete")
hc
# We can plot the dendrogram
plot(hc)
# To determine the cluster for each observation associated
# with a given cut of the dendrogram, we can use the cutree() function.
# If we want to cut horizontally:
cutree(hc, h=5)  # setting the height
# If we want to cut vertically:
cutree(hc, k=2)  # setting the number of clusters
### Elbow plot
library(factoextra)
# Elbow plot for kmeans
fviz_nbclust(pca_data, kmeans, method = "wss", k.max = 20)  +
geom_vline(xintercept = 3, linetype = 2) +
geom_vline(xintercept = 5, linetype = 2)
# Elbow plot for hierarchical clustering
fviz_nbclust(pca_data, hcut, method = "wss", k.max = 20)
# Does it make sense to consider WSS (within-cluster sum of squares (WCSS))
# in the case of Hierarchical Clustering?
# IKER: yes and no --> K-means works exactly to optimize WCSS, so the elbow plot is easier
# to interpret, this is not the case of hierarchical clustering. But still, WCSS is a
# distance measurement, so it gives information about the clusters and is OK to use.
# In case of dubious results, change the method to "silhouette"
fviz_nbclust(pca_data, hcut, method = "silhouette", k.max = 20)
# (The Silhouette score is a measure of how similar a point is
# to its own cluster compared to other clusters. The scores of all points
# are then used to compare the average silhouette score/width)
# Here some info about the Silhouette method:
# https://www.baeldung.com/cs/silhouette-values-clustering
# https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html
## Clustering interpretation ##
library(dplyr)
library(ggplot2)
library(tidyr)
# Add cluster info to original data
clusters <- km.out_1$cluster
USArrests$cluster <- as.factor(clusters)
# Compute mean per cluster
USArrests %>%
group_by(cluster) %>%
summarise(across(everything(), mean))
# We compute mean per cluster
data %>%
group_by(cluster) %>%
summarise(across(where(is.numeric), mean), .groups = "drop")
