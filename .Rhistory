)
km.out_2 <- kmeans(pca_data,
k,
nstart = 10 # how many initial conditions are created
)
k <- 3
km.out_1 <- kmeans(pca_data,
k,
nstart = 10 # how many initial conditions are created
)
km.out_2 <- kmeans(pca_data,
k,
nstart = 10 # how many initial conditions are created
)
par(mfrow = c(2, 1))
plot(pca_data, col = (km.out_1$cluster + 1), ylim = c(-3,2),
main = paste("K-Means Clustering Results with K = ", k, sep = " "),
xlab = "", ylab = "", pch = 20, cex = 2)
plot(pca_data, col = (km.out_2$cluster + 1), ylim = c(-3,2),
main = paste("K-Means Clustering Results with K = ", k, sep = " "),
xlab = "", ylab = "", pch = 20, cex = 2)
# show again the observations as function of the PCs, using the clusters
# from k-means and showing the names of the states
par(mfrow = c(1, 1))
plot(pca_data, col = (km.out_1$cluster + 1), ylim = c(-3,2),
main = paste("K-Means Clustering Results with K = ", k, sep = " "),
xlab = "", ylab = "", pch = 20, cex = 2)
# Add labels to every point
text(pca_data, pos = 1, labels = row.names(USArrests), cex = .5)
par(mfrow = c(2, 1))
plot(pca_data, col = (km.out_1$cluster + 1), ylim = c(-3,2),
main = paste("K-Means Clustering Results with K = ", k, sep = " "),
xlab = "", ylab = "", pch = 20, cex = 2)
plot(pca_data, col = (km.out_2$cluster + 1), ylim = c(-3,2),
main = paste("K-Means Clustering Results with K = ", k, sep = " "),
xlab = "", ylab = "", pch = 20, cex = 2)
par(mfrow = c(1, 1))
plot(pca_data, col = (km.out_1$cluster + 1), ylim = c(-3,2),
main = paste("K-Means Clustering Results with K = ", k, sep = " "),
xlab = "", ylab = "", pch = 20, cex = 2)
# Add labels to every point
text(pca_data, pos = 1, labels = row.names(USArrests), cex = .5)
# a smarter way of running K-means is to use the parameter nstart
k <- 3
km.out_1 <- kmeans(pca_data,
k,
nstart = 100 # how many initial conditions are created
)
km.out_2 <- kmeans(pca_data,
k,
nstart = 10 # how many initial conditions are created
)
par(mfrow = c(2, 1))
plot(pca_data, col = (km.out_1$cluster + 1), ylim = c(-3,2),
main = paste("K-Means Clustering Results with K = ", k, sep = " "),
xlab = "", ylab = "", pch = 20, cex = 2)
plot(pca_data, col = (km.out_2$cluster + 1), ylim = c(-3,2),
main = paste("K-Means Clustering Results with K = ", k, sep = " "),
xlab = "", ylab = "", pch = 20, cex = 2)
km.out_1 <- kmeans(pca_data,
5,
nstart = 100 # how many initial conditions are created
)
km.out_2 <- kmeans(pca_data,
5,
nstart = 10 # how many initial conditions are created
)
par(mfrow = c(2, 1))
plot(pca_data, col = (km.out_1$cluster + 1), ylim = c(-3,2),
main = paste("K-Means Clustering Results with K = ", k, sep = " "),
xlab = "", ylab = "", pch = 20, cex = 2)
plot(pca_data, col = (km.out_2$cluster + 1), ylim = c(-3,2),
main = paste("K-Means Clustering Results with K = ", k, sep = " "),
xlab = "", ylab = "", pch = 20, cex = 2)
# a smarter way of running K-means is to use the parameter nstart
k <- 3
km.out_1 <- kmeans(pca_data,
5,
nstart = 100 # how many initial conditions are created
)
km.out_2 <- kmeans(pca_data,
5,
nstart = 1 # how many initial conditions are created
)
par(mfrow = c(2, 1))
plot(pca_data, col = (km.out_1$cluster + 1), ylim = c(-3,2),
main = paste("K-Means Clustering Results with K = ", k, sep = " "),
xlab = "", ylab = "", pch = 20, cex = 2)
plot(pca_data, col = (km.out_2$cluster + 1), ylim = c(-3,2),
main = paste("K-Means Clustering Results with K = ", k, sep = " "),
xlab = "", ylab = "", pch = 20, cex = 2)
# Setting warnings in English and loading libraries
Sys.setenv(LANG = "en")
library(FactoMineR)
# Create a data frame with the given data
brain <- data.frame(
Benign = c(23, 21, 34),
Malignant = c(9, 4, 24),
Others = c(6, 3, 17)
)
colnames(brain) <- c("Benign", "Malignant", "Others")
rownames(brain) <- c("Frontal lobe", "Temporal lobe", "Other areas")
# INTRODUCTION #
# We will use the FactoMineR library again
library(FactoMineR)
# We will work the dataset about women's work
work <- read.table("http://factominer.free.fr/bookV2/work_women.csv",
header=TRUE,row.names=1,sep=";")
# Row profiles
dd <- rbind(work,apply(work[,1: nrow(work)],2,sum))
rownames(dd)[nrow(work) + 1] <- "Mean profile"
round(prop.table(as.matrix(dd),margin=1),2)
# Col profiles
dd <- cbind(work,apply(work[,1: ncol(work)],1,sum))
colnames(dd)[ncol(work) + 1] <- "Mean profile"
round(prop.table(as.matrix(dd),margin=2),2)
work
# INTRODUCTION #
library(FactoMineR)
# We will analyse a dataset about Crime Rates in USA cities
data("USArrests")
head(USArrests)
## Clustering ##
# INTRODUCTION #
library(FactoMineR)
# We will analyse a dataset about Crime Rates in USA cities
data("USArrests")
head(USArrests)
# Our goal is to group the states
# (whose coordinates are given by the PCA) with Clustering methods
# PCA #
# Compute the PCA
pca_res <- PCA(USArrests, scale.unit=TRUE)
pca_data <- pca_res$ind$coord[,1:2]
# Interpretation
# Have a look at the correlation diagram.
# The first component (0.62) measures the rate of crime in a state
# The second component (0.25) measures the population of urban areas
# K-means #
k <- 4
km.out_1 <- kmeans(pca_data,
k,
nstart = 1 # how many initial conditions are created
)
km.out_2 <- kmeans(pca_data,
k,
nstart = 1 # how many initial conditions are created
)
# We can plot the data, with each observation colored
# according to its cluster assignment.
par(mfrow = c(2, 1))
plot(pca_data, col = (km.out_1$cluster + 1), ylim = c(-3,2),
main = paste("K-Means Clustering Results with K = ", k, sep = " "),
xlab = "", ylab = "", pch = 20, cex = 2)
plot(pca_data, col = (km.out_2$cluster + 1), ylim = c(-3,2),
main = paste("K-Means Clustering Results with K = ", k, sep = " "),
xlab = "", ylab = "", pch = 20, cex = 2)
# what do you notice in the 2 plots?
# what happens if we set the seed?
set.seed(0)
# what does this imply when we run K-means?
k <- 4
km.out_1 <- kmeans(pca_data,
k,
nstart = 1 # how many initial conditions are created
)
km.out_2 <- kmeans(pca_data,
k,
nstart = 1 # how many initial conditions are created
)
par(mfrow = c(2, 1))
plot(pca_data, col = (km.out_1$cluster + 1), ylim = c(-3,2),
main = paste("K-Means Clustering Results with K = ", k, sep = " "),
xlab = "", ylab = "", pch = 20, cex = 2)
plot(pca_data, col = (km.out_2$cluster + 1), ylim = c(-3,2),
main = paste("K-Means Clustering Results with K = ", k, sep = " "),
xlab = "", ylab = "", pch = 20, cex = 2)
# what do you notice in the 2 plots?
# a smarter way of running K-means is to use the parameter nstart
k <- 3
km.out_1 <- kmeans(pca_data,
k,
nstart = 10 # how many initial conditions are created
)
km.out_2 <- kmeans(pca_data,
k,
nstart = 10 # how many initial conditions are created
)
# nstart parameter is an argument that specifies
# the number of initial random cluster assignments to be tried.
# It is used to find the best initial set of cluster centers
# by repeatedly running the clustering process
# with different initializations and selecting the one with
# the lowest total within-cluster sum of squares (WCSS).
#
# Note: The default value for nstart is 1, meaning the algorithm
# is run only once with a single set of initial cluster centers.
# We can increase nstart to a larger number (e.g., 10, 20, etc.)
# to improve the chances of finding a better clustering solution.
par(mfrow = c(2, 1))
plot(pca_data, col = (km.out_1$cluster + 1), ylim = c(-3,2),
main = paste("K-Means Clustering Results with K = ", k, sep = " "),
xlab = "", ylab = "", pch = 20, cex = 2)
plot(pca_data, col = (km.out_2$cluster + 1), ylim = c(-3,2),
main = paste("K-Means Clustering Results with K = ", k, sep = " "),
xlab = "", ylab = "", pch = 20, cex = 2)
# show again the observations as function of the PCs, using the clusters
# from k-means and showing the names of the states
par(mfrow = c(1, 1))
plot(pca_data, col = (km.out_1$cluster + 1), ylim = c(-3,2),
main = paste("K-Means Clustering Results with K = ", k, sep = " "),
xlab = "", ylab = "", pch = 20, cex = 2)
# Add labels to every point
text(pca_data, pos = 1, labels = row.names(USArrests), cex = .5)
# Here the observations can be easily plotted because they are two-dimensional.
# If there were more than two variables then we could instead perform PCA
# and plot the first two principal components score vectors.
## Hierarchical clustering ##
# We first have to compute a Distance Matrix, where
# we can choose among different distances, e.g. euclidean, manhattan:
distance_matrix <- dist(pca_data, method = "euclidean")
distance_matrix
zv
zsf
dg
dg
dgx
# To determine the cluster for each observation associated
# with a given cut of the dendrogram, we can use the cutree() function.
# If we want to cut horizontally:
cutree(hc, h=5)  # setting the height
# If we want to cut vertically:
cutree(hc, k=2)  # setting the number of clusters
## Hierarchical clustering ##
# We first have to compute a Distance Matrix, where
# we can choose among different distances, e.g. euclidean, manhattan:
distance_matrix <- dist(pca_data, method = "euclidean")
distance_matrix
distance_matrix
# Then, we apply the hclust function choosing the method (complete/single)
hc <-  hclust(distance_matrix, method = "complete")
hc
# We can plot the dendrogram
plot(hc)
# To determine the cluster for each observation associated
# with a given cut of the dendrogram, we can use the cutree() function.
# If we want to cut horizontally:
cutree(hc, h=5)  # setting the height
# If we want to cut vertically:
cutree(hc, k=2)  # setting the number of clusters
### Elbow plot
library(factoextra)
# Elbow plot for kmeans
fviz_nbclust(pca_data, kmeans, method = "wss", k.max = 20)  +
geom_vline(xintercept = 3, linetype = 2) +
geom_vline(xintercept = 5, linetype = 2)
# Elbow plot for hierarchical clustering
fviz_nbclust(pca_data, hcut, method = "wss", k.max = 20)
# In case of dubious results, change the method to "silhouette"
fviz_nbclust(pca_data, hcut, method = "silhouette", k.max = 20)
## Clustering interpretation ##
library(dplyr)
library(ggplot2)
library(tidyr)
# Add cluster info to original data
clusters <- km.out_1$cluster
USArrests$cluster <- as.factor(clusters)
# Compute mean per cluster
USArrests %>%
group_by(cluster) %>%
summarise(across(everything(), mean))
cluster_colors <- scico(3, palette = "lajolla")
library(scico)
cluster_colors <- scico(3, palette = "lajolla")
library(scico)
cluster_colors <- scico(3, palette = "lajolla")
# Violin plot, one column per variable, different colors for each cluster
ggplot(data_long, aes(x = cluster, y = value, fill = cluster)) +
geom_violin(trim = FALSE, alpha = 0.75) +
facet_wrap(~variable, scales = "free", ncol = 4) +  # Adjust ncol to your liking
scale_fill_manual(values = cluster_colors) +        # Apply color palette
theme_minimal() +
labs(x = "Cluster", y = "", title = "Variable Distribution per Cluster") +
theme(legend.position = "none",
strip.text = element_text(face = "bold"))
# Violin plot, one column per variable, different colors for each cluster
ggplot(data_long, aes(x = cluster, y = value, fill = cluster)) +
geom_violin(trim = FALSE, alpha = 0.75) +
facet_wrap(~variable, scales = "free", ncol = 4) +  # Adjust ncol to your liking
scale_fill_manual(values = cluster_colors) +        # Apply color palette
theme_minimal() +
labs(x = "Cluster", y = "", title = "Variable Distribution per Cluster") +
theme(legend.position = "none",
strip.text = element_text(face = "bold"))
# reshape data
data_long <- USArrests %>%
pivot_longer(cols = -cluster,
names_to = "variable",
values_to = "value")
library(scico)
cluster_colors <- scico(3, palette = "lajolla")
# Violin plot, one column per variable, different colors for each cluster
ggplot(data_long, aes(x = cluster, y = value, fill = cluster)) +
geom_violin(trim = FALSE, alpha = 0.75) +
facet_wrap(~variable, scales = "free", ncol = 4) +  # Adjust ncol to your liking
scale_fill_manual(values = cluster_colors) +        # Apply color palette
theme_minimal() +
labs(x = "Cluster", y = "", title = "Variable Distribution per Cluster") +
theme(legend.position = "none",
strip.text = element_text(face = "bold"))
# Are these results consistent with what we already knew?
plot(pca_res,choix = "var")
vars <- USArrests[, sapply(USArrests, is.numeric)]
n_clusters <- length(unique(clusters))
# Matrix to store correlations
cor_matrix <- matrix(NA, nrow = ncol(vars), ncol = n_clusters)
rownames(cor_matrix) <- colnames(vars)
colnames(cor_matrix) <- paste0("Cluster_", 1:n_clusters)
# Compute correlations
for (k in 1:n_clusters) {
cluster_bin <- as.numeric(clusters == k)
cor_matrix[, k] <- sapply(vars, function(x) cor(x, cluster_bin))
}
cor_matrix
setwd("~/UNI/2_SEMESTRE/ADVANCED STATISTICS/PROJECT/cancerResearch")
# We load the libraries we will used to create the model:
# install.packages("moments") # install if needed
library(dplyr)
library(pastecs)
library(ggplot2)
library(lattice)
library(car)
library(moments)
data <- read.csv("./breast_cancer_data.csv") # We read the dataset from the csv file
head(data) # We visualize the first six rows of the dataset
str(data) # We visualize the structure of the data (the different variables, their type and some initial values)
# First we plot the boxplots for the radius mean based on the diagnosis
ggplot(data, aes(x = diagnosis , y = radius_mean, fill = diagnosis)) +
geom_boxplot() +
ggtitle("Comparison of Radius Mean for Benign vs Malignant Tumors")
# We create the linear model with all the predictors that can be included taking into account our analysis
mod1 <- lm(radius_mean~texture_mean+smoothness_mean+compactness_mean
+concave.points_mean+symmetry_mean+fractal_dimension_mean
+texture_se+smoothness_se+symmetry_se, data = data)
# Check the pvalue
mod1s = summary(mod1)
mod1s # The pvalue is very small (<2.2e-16)
# Prediction with the data used to train the model (confidence interval) #
confidence_interval <- data.frame(radius_mean = data_train$radius_mean)
confidence_interval <- cbind(confidence_interval, as.data.frame(
predict(mod13, data_train, interval = "confidence", level = 0.99)))
# LINEAR REGRESSION MODEL OF THE WISCONSIN BREAST CANCER DATASET
# Authors: Iker Ibarrola, Enetz Quindimil, Carlos Firvida, Iñigo Infante
# We load the libraries we will used to create the model:
# install.packages("moments") # install if needed
library(dplyr)
library(pastecs)
library(ggplot2)
library(lattice)
library(car)
library(moments)
###################################
# DATASET LOADING & CHECKING
###################################
data <- read.csv("./breast_cancer_data.csv") # We read the dataset from the csv file
head(data) # We visualize the first six rows of the dataset
str(data) # We visualize the structure of the data (the different variables, their type and some initial values)
###################################
# DATASET MODIFICATION
###################################
# We make some of the modificatons previously done in the descriptive analysis part:
data <- data %>% select(-id) # We remove id
data$diagnosis <- factor(data$diagnosis) # Create a factor from the diagnosis variabl:
levels(data$diagnosis) <- c("benign", "malign") # Instead of using 'b' & 'm', use more descriptive labels
summary(data$diagnosis) # We check that previously steps worked properly
# We divide the dataset into two parts: one to train the model (80%) and the other to test it (20%)
data_test <- data[473:569, ] # Data to test the model
data_train <- data[1:472, ] # Data to test the model
###################################
# RESPONSE SELECTION
###################################
# We have selected the Radius Mean as the response to calculate. This decisions has been made after
# reviewing again the descriptive analysis and confirming that the radius mean is a good parameter
# to classify a tumor as Benign or Malign (We know this will be done in Logistic Regression, but we
# understand that the highest-level goal of such a study is to differentiate the diagnosis of the tumor,
# and as the Radius Mean does it, the selection of the response follows that logic)
# This is how we have seen that radius mean is important for predicting the diagnosis:
# First we plot the boxplots for the radius mean based on the diagnosis
ggplot(data, aes(x = diagnosis , y = radius_mean, fill = diagnosis)) +
geom_boxplot() +
ggtitle("Comparison of Radius Mean for Benign vs Malignant Tumors")
# Then we perform an ANOVA test using the function aov() (not anova()), to see whether there is a
# significant difference in the variation within the groups and between the groups.
# Fit the ANOVA model
model_aov <- aov(radius_mean ~ diagnosis, data = data)
# Check overall significance
anova_result <- summary(model_aov)
p_value <- anova_result[[1]]$"Pr(>F)"[1]
p_value # The p-value is 8.465941e-96, confirming that the is a significant difference between
# the radius mean for each diagnosis group.
# Thus, the selection of Radius Mean as the response makes sense taking into account our objective
###################################
# PREDICTOR SELECTION
###################################
# We start by thinking what variables can't be selected as predictors, taking into account the
# correlations calculated during the descriptive analysis (obviously we are not going to use the diagnosis
# as a predictor, since we have stated that our final objective is to calculate it from the radius):
# radius_se & radius_worst: as they are other measurements of the response
# variables regarding area & perimeter: as they are mathematically dependent on radius (obviously related)
# x_worst: as they are highly correlated (over 0.7) with x_mean, introducing multicollinearity
# some x_se: as they are correlated with x_mean, introducing multicollinearity
# The ones with a correlation below 0.5 are: texture, smoothness & symmetry.
# concavity_mean & concave.points_mean: we can't select both as they are highly correlated (over 0.9).
# Thus, we choose the one more correlated to radius_mean, in this case concave.points_mean.
cor(data$radius_mean, data$concavity_mean)
cor(data$radius_mean, data$concave.points_mean)
# *x_se, x_worst & x_mean refer to the generalised way of expressing all the variables including
# se, worst or mean
# We create the linear model with all the predictors that can be included taking into account our analysis
mod1 <- lm(radius_mean~texture_mean+smoothness_mean+compactness_mean
+concave.points_mean+symmetry_mean+fractal_dimension_mean
+texture_se+smoothness_se+symmetry_se, data = data)
###################################
#### Test of all predictors (F test)
# Check the pvalue
mod1s = summary(mod1)
mod1s # The pvalue is very small (<2.2e-16)
# We can compute the pvalue
# TSS
TSS <- sum((data$radius_mean-mean(data$radius_mean))^2)
# RSS
RSS <- deviance(mod1)
# F statistic
Fstat <- ((TSS-RSS)/(9)) / (RSS/df.residual(mod1))
Fstat
# F_alpha(p,n-(p+1)) # setting alpha = 0.05
F_alpha = qf(0.05, 9, 559, lower.tail = FALSE) # n-p(+1) = df.residual(mod1) = dim(x)[2]-1
# Reject H0 if Fstat > F_alpha
Fstat > F_alpha
# to obtain the pvalue we have to compute P(F_alpha>Fstat) to obtain the pvalue
pf(Fstat, 9, df.residual(mod1), lower.tail = FALSE)
# Given that the pvalue is very small the null hypothesis is rejected, meaning that the model with
# 9 variables is better than the model given by the average.
###################################
#### Test of a single predictor (T-Test)
# We just have to check the t statistic for each predictor
mod1s # High pvalues mean that the null hypothesis is not rejected and that the predictor is not
# relevant for the model
# It can also be computed directly
Tstat <- mod1s$coefficients[,1]/mod1s$coefficients[,2]
# this computes the estimator divided by the standard error, i.e., the statistic
Tstat
# t_alpha/2(n-(p+1)) # setting alpha = 0.05
t_alpha = qt(0.05/2, mod1$df.residual, lower.tail = FALSE)
# Reject H0 if
abs(Tstat) > t_alpha
# to obtain the pvalue we have to compute P(|t|>Tstat) to obtain the pvalue
pval_t = 2*pt(abs(Tstat), mod1$df.residual, lower.tail = FALSE)
pval_t
###################################
#### Test of a subset of predictors / nested models (ANOVA Test)
# We select a subset of predictors based on the previously computed T-Test
mod_nested <- lm(radius_mean~smoothness_mean+compactness_mean
+concave.points_mean+symmetry_mean+fractal_dimension_mean
+smoothness_se, data = data)
# In this case, H0 is that both models perform similarly, while H1 is that they perform differently
anova(mod_nested,mod1) # As the pvalue is large, we cannot reject H0, meaning that We can consider
# the simpler model, since they perform similarly
###################################
# MODEL SELECTION
###################################
# Having in mind the restrictions mentioned in the predictor selection part, we will start the predictor
# selection using Backward Elimination:
# First we select alpha = 0.05 as the significance level
# We create the first model, which will contain all the variables that haven't been logically discarded.
mod10 <- lm(radius_mean~texture_mean+smoothness_mean+compactness_mean
+concave.points_mean+symmetry_mean+fractal_dimension_mean
+texture_se+smoothness_se+symmetry_se, data = data_train)
summary(mod10)
# The symmetry_se has the highest pvalue higher than the critical alpha
# We delete symmetry_se and create a new model
mod11 <- lm(radius_mean~texture_mean+smoothness_mean+compactness_mean
+concave.points_mean+symmetry_mean+fractal_dimension_mean
+texture_se+smoothness_se, data = data_train)
summary(mod11) # The R squared is the same even though we have removed a variable
# Now texture_se has the highest pvalue above the threshold
# We delete texture_se and create a new model
mod12 <- lm(radius_mean~smoothness_mean+compactness_mean
+concave.points_mean+symmetry_mean+fractal_dimension_mean
+texture_mean+smoothness_se, data = data_train)
summary(mod12)# The R squared is almost the same after removing another variable
# Now texture_mean has the highest pvalue above the threshold
# We delete texture_mean and create a new model
mod13 <- lm(radius_mean~smoothness_mean+compactness_mean
+concave.points_mean+symmetry_mean+fractal_dimension_mean
+smoothness_se, data = data_train)
summary(mod13)# The R squared is almost the same after removing another variable
###################################
# We perform a quick diagnosis to confirm everything is correct
plot(mod13)
# The new model without that observations
mod20 <- lm(radius_mean~smoothness_mean+compactness_mean+concave.points_mean
+symmetry_mean+fractal_dimension_mean+smoothness_se, data = new_data)
# Prediction with the data used to train the model (confidence interval) #
confidence_interval <- data.frame(radius_mean = data_train$radius_mean)
confidence_interval <- cbind(confidence_interval, as.data.frame(
predict(mod13, data_train, interval = "confidence", level = 0.99)))
# Add a column to check whether the predicted values is in the range or not
confidence_interval$in_range <- ifelse(confidence_interval$radius_mean > confidence_interval$lwr &
confidence_interval$radius_mean < confidence_interval$upr,
TRUE, FALSE)
confidence_interval # In this case, as we are using the data used to train the model, the SE is low
