smoothness_worst, data = data_train)
summary(mod69)
# Remove smoothness_worst
mod70 <- lm(radius_mean ~ texture_mean + smoothness_mean +
compactness_mean + concavity_mean + concave.points_mean + symmetry_mean +
fractal_dimension_mean +
compactness_se + concavity_se + concave.points_se +
fractal_dimension_se, data = data_train)
summary(mod70)
# Remove texture_mean
mod71 <- lm(radius_mean ~ smoothness_mean +
compactness_mean + concavity_mean + concave.points_mean + symmetry_mean +
fractal_dimension_mean +
compactness_se + concavity_se + concave.points_se +
fractal_dimension_se, data = data_train)
summary(mod71)
# Remove concavity_se
mod72 <- lm(radius_mean ~ smoothness_mean +
compactness_mean + concavity_mean + concave.points_mean + symmetry_mean +
fractal_dimension_mean +
compactness_se + concave.points_se +
fractal_dimension_se, data = data_train)
summary(mod72)
# Remove concavity_mean
mod73 <- lm(radius_mean ~ smoothness_mean +
compactness_mean + concave.points_mean + symmetry_mean +
fractal_dimension_mean +
compactness_se + concave.points_se +
fractal_dimension_se, data = data_train)
summary(mod73)
cor(data$compactness_mean, data$compactness_se)
cor(data$concave.points_mean, data$concave.points_se)
cor(data$fractal_dimension_mean, data$fractal_dimension_se)
# No more to remove
mod13 <- lm(radius_mean~smoothness_mean+compactness_mean
+concave.points_mean+symmetry_mean+fractal_dimension_mean
+smoothness_se, data = data_train)
summary(mod13)# The R squared is almost the same after removing another variable
# Prediction with "new" data, not the one used to train the model (prediction interval) #
prediction_interval <- data.frame(radius_mean = data_test$radius_mean)
prediction_interval <- cbind(prediction_interval, as.data.frame(
predict(mod73, data_test, interval = "prediction")))
# Add a column to check whether the predicted values is in the range or not
prediction_interval$in_range <- ifelse(prediction_interval$radius_mean > prediction_interval$lwr &
prediction_interval$radius_mean < prediction_interval$upr,
TRUE, FALSE)
prediction_interval # In this case, as there is more variability in predicting a new response, the
# We can check the percentage of predictions that were in the given range of the prediction intervals
sum(prediction_interval$in_range)/nrow(prediction_interval)
# Prediction with the data used to train the model (confidence interval) #
confidence_interval <- data.frame(radius_mean = data_train$radius_mean)
confidence_interval <- cbind(confidence_interval, as.data.frame(
predict(mod73, data_train, interval = "confidence", level = 0.99)))
# Add a column to check whether the predicted values is in the range or not
confidence_interval$in_range <- ifelse(confidence_interval$radius_mean > confidence_interval$lwr &
confidence_interval$radius_mean < confidence_interval$upr,
TRUE, FALSE)
confidence_interval # In this case, as we are using the data used to train the model, the SE is low
# We can check the percentage of predictions that were in the given range of the confidence interval
sum(confidence_interval$in_range)/nrow(confidence_interval)
# Prediction with "new" data, not the one used to train the model (prediction interval) #
prediction_interval <- data.frame(radius_mean = data_test$radius_mean)
prediction_interval <- cbind(prediction_interval, as.data.frame(
predict(mod73, data_test, interval = "prediction")))
# Add a column to check whether the predicted values is in the range or not
prediction_interval$in_range <- ifelse(prediction_interval$radius_mean > prediction_interval$lwr &
prediction_interval$radius_mean < prediction_interval$upr,
TRUE, FALSE)
prediction_interval # In this case, as there is more variability in predicting a new response, the
# We can check the percentage of predictions that were in the given range of the prediction intervals
sum(prediction_interval$in_range)/nrow(prediction_interval)
cor(data$compactness_mean, data$compactness_se)
cor(data$concave.points_mean, data$concave.points_se)
cor(data$fractal_dimension_mean, data$fractal_dimension_se)
plot(mod73)
glue("confidence_interval_perimeter is {confidence_interval_perimeter}")
glue("confidence_interval_area is {confidence_interval_area}")
glue("confidence_interval_all is {confidence_interval_all}")
# Prediction with the data used to train the model (confidence interval) #
confidence_interval <- data.frame(radius_mean = data_train$radius_mean)
confidence_interval <- cbind(confidence_interval, as.data.frame(
predict(mod73, data_train, interval = "confidence", level = 0.99)))
# Add a column to check whether the predicted values is in the range or not
confidence_interval$in_range <- ifelse(confidence_interval$radius_mean > confidence_interval$lwr &
confidence_interval$radius_mean < confidence_interval$upr,
TRUE, FALSE)
confidence_interval # In this case, as we are using the data used to train the model, the SE is low
# We can check the percentage of predictions that were in the given range of the confidence interval
confidence_interval_all <- sum(confidence_interval$in_range)/nrow(confidence_interval)
# Prediction with "new" data, not the one used to train the model (prediction interval) #
prediction_interval <- data.frame(radius_mean = data_test$radius_mean)
prediction_interval <- cbind(prediction_interval, as.data.frame(
predict(mod73, data_test, interval = "prediction")))
# Add a column to check whether the predicted values is in the range or not
prediction_interval$in_range <- ifelse(prediction_interval$radius_mean > prediction_interval$lwr &
prediction_interval$radius_mean < prediction_interval$upr,
TRUE, FALSE)
prediction_interval # In this case, as there is more variability in predicting a new response, the
# We can check the percentage of predictions that were in the given range of the prediction intervals
prediction_interval_all <- sum(prediction_interval$in_range)/nrow(prediction_interval)
glue("confidence_interval_perimeter is {confidence_interval_perimeter}")
glue("confidence_interval_area is {confidence_interval_area}")
glue("confidence_interval_all is {confidence_interval_all}")
glue("confidence_interval_perimeter is {confidence_interval_perimeter}")
glue("confidence_interval_area is {confidence_interval_area}")
glue("confidence_interval_all is {confidence_interval_all}")
# We can check the percentage of predictions that were in the given range of the prediction intervals
prediction_interval_all <- round(sum(prediction_interval$in_range)/nrow(prediction_interval),4)
# We can check the percentage of predictions that were in the given range of the confidence interval
confidence_interval_all <- round(sum(confidence_interval$in_range)/nrow(confidence_interval),4)
glue("confidence_interval_perimeter is {confidence_interval_perimeter}")
glue("confidence_interval_area is {confidence_interval_area}")
glue("confidence_interval_all is {confidence_interval_all}")
cat("(\n\n")
glue("prediction_interval_perimeter is {prediction_interval_perimeter}")
glue("prediction_interval_area is {prediction_interval_area}")
glue("prediction_interval_all is {confidence_interval_all}")
glue("prediction_interval_all is {prediction_interval_all}")
# Remove concavity_mean
mod73 <- lm(radius_mean ~ smoothness_mean +
compactness_mean + concave.points_mean + symmetry_mean +
fractal_dimension_mean, data = data_train)
summary(mod73)
###################################
#### Confidence Interval
# Prediction with the data used to train the model (confidence interval) #
confidence_interval <- data.frame(radius_mean = data_train$radius_mean)
confidence_interval <- cbind(confidence_interval, as.data.frame(
predict(mod73, data_train, interval = "confidence", level = 0.99)))
# Add a column to check whether the predicted values is in the range or not
confidence_interval$in_range <- ifelse(confidence_interval$radius_mean > confidence_interval$lwr &
confidence_interval$radius_mean < confidence_interval$upr,
TRUE, FALSE)
confidence_interval # In this case, as we are using the data used to train the model, the SE is low
# We can check the percentage of predictions that were in the given range of the confidence interval
confidence_interval_all <- round(sum(confidence_interval$in_range)/nrow(confidence_interval),4)
###################################
#### Prediction Interval
# Prediction with "new" data, not the one used to train the model (prediction interval) #
prediction_interval <- data.frame(radius_mean = data_test$radius_mean)
prediction_interval <- cbind(prediction_interval, as.data.frame(
predict(mod73, data_test, interval = "prediction")))
# Add a column to check whether the predicted values is in the range or not
prediction_interval$in_range <- ifelse(prediction_interval$radius_mean > prediction_interval$lwr &
prediction_interval$radius_mean < prediction_interval$upr,
TRUE, FALSE)
prediction_interval # In this case, as there is more variability in predicting a new response, the
# SE (difference between lower and upper) is larger than in the case of the confidence interval
# We can check the percentage of predictions that were in the given range of the prediction intervals
prediction_interval_all <- round(sum(prediction_interval$in_range)/nrow(prediction_interval),4)
###################################
# DISPLAYING THE RESULTS
glue("confidence_interval_all is {confidence_interval_all}")
glue("prediction_interval_all is {prediction_interval_all}")
cat("(\n\n\n\n")
glue("confidence_interval_perimeter is {confidence_interval_perimeter}")
glue("confidence_interval_area is {confidence_interval_area}")
glue("confidence_interval_all is {confidence_interval_all}")
cat("(\n\n")
glue("prediction_interval_perimeter is {prediction_interval_perimeter}")
glue("prediction_interval_area is {prediction_interval_area}")
glue("prediction_interval_all is {prediction_interval_all}")
# Remove concavity_mean
mod73 <- lm(radius_mean ~ smoothness_mean +
compactness_mean + concave.points_mean + symmetry_mean +
fractal_dimension_mean +
compactness_se + concave.points_se +
fractal_dimension_se, data = data_train)
summary(mod73)
plot(mod73)
###################################
#### Confidence Interval
# Prediction with the data used to train the model (confidence interval) #
confidence_interval <- data.frame(radius_mean = data_train$radius_mean)
confidence_interval <- cbind(confidence_interval, as.data.frame(
predict(mod73, data_train, interval = "confidence", level = 0.99)))
# Add a column to check whether the predicted values is in the range or not
confidence_interval$in_range <- ifelse(confidence_interval$radius_mean > confidence_interval$lwr &
confidence_interval$radius_mean < confidence_interval$upr,
TRUE, FALSE)
confidence_interval # In this case, as we are using the data used to train the model, the SE is low
# We can check the percentage of predictions that were in the given range of the confidence interval
confidence_interval_all <- round(sum(confidence_interval$in_range)/nrow(confidence_interval),4)
###################################
#### Prediction Interval
# Prediction with "new" data, not the one used to train the model (prediction interval) #
prediction_interval <- data.frame(radius_mean = data_test$radius_mean)
prediction_interval <- cbind(prediction_interval, as.data.frame(
predict(mod73, data_test, interval = "prediction")))
# Add a column to check whether the predicted values is in the range or not
prediction_interval$in_range <- ifelse(prediction_interval$radius_mean > prediction_interval$lwr &
prediction_interval$radius_mean < prediction_interval$upr,
TRUE, FALSE)
prediction_interval # In this case, as there is more variability in predicting a new response, the
# SE (difference between lower and upper) is larger than in the case of the confidence interval
# We can check the percentage of predictions that were in the given range of the prediction intervals
prediction_interval_all <- round(sum(prediction_interval$in_range)/nrow(prediction_interval),4)
###################################
# DISPLAYING THE RESULTS
###################################
cat("(\n\n\n\n")
glue("confidence_interval_perimeter is {confidence_interval_perimeter}")
glue("confidence_interval_area is {confidence_interval_area}")
glue("confidence_interval_all is {confidence_interval_all}")
cat("(\n\n")
glue("prediction_interval_perimeter is {prediction_interval_perimeter}")
glue("prediction_interval_area is {prediction_interval_area}")
glue("prediction_interval_all is {prediction_interval_all}")
# We can see that the results are extremely similar, meaning that there is not so much difference
# between predicting the perimeter or the area
# Finally, if we recall the values for the model predicting the radius, this were the results:
# Confidence_interval_radius: 0.2033
# Prediction_interval_radius: 0.9381
# Once again, extremely similar values, concluding that the three of them predict the response
# very similarly
mod60 <- lm(radius_mean ~ texture_mean + smoothness_mean +
compactness_mean + concavity_mean + concave.points_mean + symmetry_mean +
fractal_dimension_mean + texture_se +
smoothness_se + compactness_se + concavity_se + concave.points_se + symmetry_se +
fractal_dimension_se + texture_worst +
smoothness_worst + compactness_worst + concavity_worst + concave.points_worst +
symmetry_worst + fractal_dimension_worst, data = data_train)
summary(mod60)
cls
###################################
# We perform a diagnosis to confirm everything is correct
plot(mod13)
summary(mod13)
summary(mod20)
# We check BIC & AIC to see if our final model is the best
AIC(mod10, mod11, mod12, mod13)
BIC(mod10, mod11, mod12, mod13)
# We compare the summary statistics
summary(mod13)
# We will carry out the model diagnosis visually by plotting the model (we will use the selected model)
plot(mod13)
# We will carry out the model diagnosis visually by plotting the model (we will use the selected model)
plot(mod13)
# We visualize the first plot (residual plot) to check the residuals against the fitted values
# This allows us to see if the variance is constant across the error terms (homoscedasticity)
# and that the errors are symmetric vertically about zero (linearity)
plot(mod13, 1) # The plot shows that both characteristics are fulfilled, meaning that the model
# We visualize the second plot (Q-Q plot) to check the distribution of the error terms
plot(mod13, 2) # We can see that it follows a normal distribution
# We create the first model, which will contain all the variables that haven't been logically discarded.
mod10 <- lm(radius_mean~texture_mean+smoothness_mean+compactness_mean
+concave.points_mean+symmetry_mean+fractal_dimension_mean
+texture_se+smoothness_se+symmetry_se, data = data_train)
summary(mod10)
# We delete symmetry_se and create a new model
mod11 <- lm(radius_mean~texture_mean+smoothness_mean+compactness_mean
+concave.points_mean+symmetry_mean+fractal_dimension_mean
+texture_se+smoothness_se, data = data_train)
summary(mod11) # The R squared is the same even though we have removed a variable
# We delete texture_se and create a new model
mod12 <- lm(radius_mean~smoothness_mean+compactness_mean
+concave.points_mean+symmetry_mean+fractal_dimension_mean
+texture_mean+smoothness_se, data = data_train)
summary(mod12)# The R squared is almost the same after removing another variable
# We delete texture_mean and create a new model
mod13 <- lm(radius_mean~smoothness_mean+compactness_mean
+concave.points_mean+symmetry_mean+fractal_dimension_mean
+smoothness_se, data = data_train)
summary(mod13)# The R squared is almost the same after removing another variable
# We visualize the second plot (Q-Q plot) to check the distribution of the error terms
plot(mod13, 2) # We can see that it follows a normal distribution
shapiro.test(residuals(mod13))
shapiro.test(residuals(mod13))
# We check high leverage points by computing Cook's distance
sort(cooks.distance(mod13))
# We visualize the fifth plot (residual vs leverage points)
plot(mod13, 5) # We see that most point have a very low leverage, except for 214. Still, it is
# However, we try computing the model without that particular observation
new_data_1 <- data[-214, ] # Excluding the point with highest leverage
# We create the new new model
mod21 <- lm(radius_mean~smoothness_mean+compactness_mean+concave.points_mean
+symmetry_mean+fractal_dimension_mean+smoothness_se, data = new_data_1)
# We create the new new model
mod21 <- lm(radius_mean~smoothness_mean+compactness_mean+concave.points_mean
+symmetry_mean+fractal_dimension_mean+smoothness_se, data = new_data_1)
shapiro.test(residuals(mod21))
cks
shapiro.test(residuals(mod21))
shapiro.test(residuals(mod13))
# We create the new new model
mod21 <- lm(sqrt(radius_mean)~smoothness_mean+compactness_mean+concave.points_mean
+symmetry_mean+fractal_dimension_mean+smoothness_se, data = new_data_1)
summary(mod21) # Observation 214 excluded
# We compare the two models (with and without observation number 214)
summary(mod13) # Observation 214 included
# We compare the two plots (with and without observation number 214)
plot(mod13, 5)
plot(mod21, 5)
# However, we try computing the model without that particular observation
new_data_1 <- data[-214, ] # Excluding the point with highest leverage
# We create the new new model
mod21 <- lm(sqrt(radius_mean)~smoothness_mean+compactness_mean+concave.points_mean
+symmetry_mean+fractal_dimension_mean+smoothness_se, data = new_data_1)
# We compare the two plots (with and without observation number 214)
plot(mod13, 5)
plot(mod21, 5)
shapiro.test(mod13)
shapiro.test(residuals(mod13))
shapiro.test(residuals(mod21))
# We create the new new model
mod21 <- lm(radius_mean^2~smoothness_mean+compactness_mean+concave.points_mean
+symmetry_mean+fractal_dimension_mean+smoothness_se, data = new_data_1)
shapiro.test(residuals(mod21))
summary(mod21) # Observation 214 excluded
residuals
shapiro.test(residuals(mod21))
plot(mod21, 2)
shapiro.test(residuals(mod21))
plot(mod21, 2)
# We create the new new model
mod21 <- lm(sqrt(radius_mean)~smoothness_mean+compactness_mean+concave.points_mean
+symmetry_mean+fractal_dimension_mean+smoothness_se, data = new_data_1)
shapiro.test(residuals(mod21))
plot(mod21, 2)
# We create the new new model
mod21 <- lm(log(radius_mean)~smoothness_mean+compactness_mean+concave.points_mean
+symmetry_mean+fractal_dimension_mean+smoothness_se, data = new_data_1)
shapiro.test(residuals(mod21))
plot(mod21, 2)
shapiro.test(residuals(mod13))
# We create the new new model
mod21 <- lm(log(radius_mean)~smoothness_mean+compactness_mean+concave.points_mean
+symmetry_mean+fractal_dimension_mean+smoothness_se, data = new_data_1)
shapiro.test(residuals(mod21))
plot(mod21, 2)
library(nortest)
install.packages("nortest")
library(nortest)
ad.test(residuals(mod13))  # Anderson-Darling test (better for large datasets)
ad.test(residuals(mod21))  # Anderson-Darling test (better for large datasets)
hist(residual(mod13))
hist(residuals(mod13))
hist(residuals(mod13), breaks = 10)
hist(residuals(mod13), breaks = 10)
hist(residuals(mod13), breaks = 10)
hist?
###################################
### Unusual observations
# We check high leverage points by computing Cook's distance
sort(cooks.distance(mod13))
hist?
hist()?
###################################
### Unusual observations
# We check high leverage points by computing Cook's distance
sort(cooks.distance(mod13))
?hist
resid_vals <- residuals(mod13)  # Extract residuals
hist(resid_vals, probability = TRUE, main = "Histogram of Residuals with Normal Curve",
xlab = "Residuals", col = "lightblue", border = "black")
# Add normal density curve
curve(dnorm(x, mean = mean(resid_vals), sd = sd(resid_vals)),
col = "red", lwd = 2, add = TRUE)
hist(resid_vals, probability = TRUE, main = "Histogram of Residuals with Normal Curve",
xlab = "Residuals", col = "lightblue", border = "black", breaks = 20)
hist(resid_vals, probability = TRUE, main = "Histogram of Residuals with Normal Curve",
xlab = "Residuals", col = "lightblue", border = "black", breaks = 100)
# We visualize the second plot (Q-Q plot) to check the distribution of the error terms
plot(mod13, 2) # We can see that it follows a normal distribution
# We delete texture_mean and create a new model
mod13 <- lm(radius_mean~smoothness_mean+compactness_mean
+concave.points_mean+symmetry_mean+fractal_dimension_mean
+smoothness_se, data = data)
summary(mod13)# The R squared is almost the same after removing another variable
plot(mod13, 2)
# We delete texture_mean and create a new model
mod13 <- lm(radius_mean~smoothness_mean+compactness_mean
+concave.points_mean+symmetry_mean+fractal_dimension_mean
+smoothness_se, data = data)
summary(mod13)# The R squared is almost the same after removing another variable
plot(mod13, 2)
# We delete texture_mean and create a new model
mod13 <- lm(radius_mean~smoothness_mean+compactness_mean
+concave.points_mean+symmetry_mean+fractal_dimension_mean
+smoothness_se, data = data_train)
summary(mod13)# The R squared is almost the same after removing another variable
# We visualize the second plot (Q-Q plot) to check the distribution of the error terms
plot(mod13, 2) # We can see that it follows a normal distribution
hist(x = residuals(mod13), breaks = 15)
hist(x = residuals(mod13), breaks = 20)
hist(x = residuals(mod13), breaks = 20)
hist(x = residuals(mod13), breaks = 25)
hist(x = residuals(mod13), breaks = 25)
shapiro.test(residuals(mod13))
hist(x = residuals(mod13), breaks = 25)
hist(x = residuals(mod13), breaks = 100)
hist(x = residuals(mod13), breaks = 50)
hist(x = residuals(mod13), breaks = 30)
hist(x = residuals(mod13), breaks = 20)
hist(x = residuals(mod13), breaks = 20)
# Generate some sample data
data <- rnorm(1000, mean = 50, sd = 10)
# Add the normal distribution curve
x_vals <- seq(min(data), max(data), length = 100)
y_vals <- dnorm(x_vals, mean = mean(data), sd = sd(data))
lines(x_vals, y_vals, col = "red", lwd = 2)
# We check high leverage points by computing Cook's distance
sort(cooks.distance(mod13))
# Create the histogram of residuals
hist(residuals(mod13), breaks = 20, probability = TRUE, col = "lightblue", main = "Histogram of Residuals with Normal Distribution", xlab = "Residuals", ylab = "Density")
# Add the normal distribution curve
x_vals <- seq(min(residuals(mod13)), max(residuals(mod13)), length = 100)
y_vals <- dnorm(x_vals, mean = mean(residuals(mod13)), sd = sd(residuals(mod13)))
lines(x_vals, y_vals, col = "red", lwd = 2)
# Create the histogram of residuals
hist(residuals(mod13), breaks = 30, probability = TRUE, col = "lightblue", main = "Histogram of Residuals with Normal Distribution", xlab = "Residuals", ylab = "Density")
# Add the normal distribution curve
x_vals <- seq(min(residuals(mod13)), max(residuals(mod13)), length = 100)
y_vals <- dnorm(x_vals, mean = mean(residuals(mod13)), sd = sd(residuals(mod13)))
lines(x_vals, y_vals, col = "red", lwd = 2)
# Create the histogram of residuals
hist(residuals(mod13), breaks = 100, probability = TRUE, col = "lightblue", main = "Histogram of Residuals with Normal Distribution", xlab = "Residuals", ylab = "Density")
# Add the normal distribution curve
x_vals <- seq(min(residuals(mod13)), max(residuals(mod13)), length = 100)
y_vals <- dnorm(x_vals, mean = mean(residuals(mod13)), sd = sd(residuals(mod13)))
lines(x_vals, y_vals, col = "red", lwd = 2)
# Create the histogram of residuals
hist(residuals(mod13), breaks = 10, probability = TRUE, col = "lightblue", main = "Histogram of Residuals with Normal Distribution", xlab = "Residuals", ylab = "Density")
# Add the normal distribution curve
x_vals <- seq(min(residuals(mod13)), max(residuals(mod13)), length = 100)
y_vals <- dnorm(x_vals, mean = mean(residuals(mod13)), sd = sd(residuals(mod13)))
lines(x_vals, y_vals, col = "red", lwd = 2)
# Create the histogram of residuals
hist(residuals(mod13), breaks = 20, probability = TRUE, col = "lightblue", main = "Histogram of Residuals with Normal Distribution", xlab = "Residuals", ylab = "Density")
# Add the normal distribution curve
x_vals <- seq(min(residuals(mod13)), max(residuals(mod13)), length = 100)
y_vals <- dnorm(x_vals, mean = mean(residuals(mod13)), sd = sd(residuals(mod13)))
lines(x_vals, y_vals, col = "red", lwd = 2)
# We visualize the second plot (Q-Q plot) to check the distribution of the error terms
plot(mod13, 2) # We can see that it follows a normal distribution
# We visualize the second plot (Q-Q plot) to check the distribution of the error terms
plot(mod13, 2) # Even though it is not perfectly normal, it's not considerably skewed or tailed, so
# We check high leverage points by computing Cook's distance
sort(cooks.distance(mod13))
cls
# we can consider that it follows a normal distribution
library(moments)
# we can consider that it follows a normal distribution
install.packages("moments")
library(moments)
skewness(residuals(lm_model))
# we can consider that it follows a normal distribution
install.packages("moments")
library(moments)
skewness(residuals(mod13))
skewness(residuals(mod13))
kurtosis(residuals(mod13))
# Note:
set.seed (6)
x <- rnorm (1000 ,0 ,1)
shapiro.test(x)
x <- rnorm (10000 ,0 ,1)
shapiro.test(x)
# Note:
set.seed (6)
x <- rnorm (10000 ,0 ,1)
shapiro.test(x)
x <- rnorm (5000 ,0 ,1)
shapiro.test(x)
# Note:
set.seed (6)
x <- rnorm (5000 ,0 ,1)
shapiro.test(x)
# Note:
set.seed (6)
x <- rnorm (5000 ,0 ,1)
shapiro.test(x)
# Note:
set.seed (6)
x <- rnorm (500 ,0 ,1)
shapiro.test(x)
# Note:
set.seed (6)
x <- rnorm (500 ,0 ,1)
shapiro.test(x)
# Note:
set.seed (6)
x <- rnorm (500 ,0 ,1)
shapiro.test(x)
# Note:
set.seed (6)
x <- rnorm (500 ,0 ,1)
shapiro.test(x)
# Note:
x <- rnorm (500 ,0 ,1)
shapiro.test(x)
# Note:
x <- rnorm (500 ,0 ,1)
shapiro.test(x)
# Note:
x <- rnorm (500 ,0 ,1)
shapiro.test(x)
# Note:
x <- rnorm (500 ,0 ,1)
shapiro.test(x)
# Note:
x <- rnorm (500 ,0 ,1)
shapiro.test(x)
# Note:
x <- rnorm (500 ,0 ,1)
shapiro.test(x)
# Note:
x <- rnorm (500 ,0 ,1)
shapiro.test(x)
# Note:
x <- rnorm (500 ,0 ,1)
shapiro.test(x)
