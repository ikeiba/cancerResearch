# Scatter plot of radius_mean vs perimeter_mean
ggplot(data, aes(x = radius_mean, y = concavity_mean, color = diagnosis)) +
geom_point() +
ggtitle("Scatter Plot of Radius Mean vs Perimeter Mean")
# We continue by using the contingency table to check some information about benign and malign tumors
table(data$diagnosis)
# We look at the proportion of benign vs malignant cases
prop.table(table(data$diagnosis))
# We use a bar plot to visualize this
ggplot(data)+
aes(diagnosis)+
geom_bar()
# Scatter plot of smoothness_mean vs perimeter_mean
ggplot(data, aes(x = smoothness_mean, y = compactness_mean, color = diagnosis)) +
geom_point() +
ggtitle("Scatter Plot of Radius Mean vs Perimeter Mean")
# We end by using some other charts/plots
# qq plot
qqnorm(data$radius_mean)
qqline(data$radius_mean)
setwd("~/UNI/2_SEMESTRE/ADVANCED STATISTICS/UNIT 1")
# Descriptive Statistics in R
# This code performs an initial data analysis using a dataset
# from the National Institute of Diabetes and Digestive and Kidney diseases,
# it is a study on 768 adult female Pima Indians living near Phoenix.
# The data may be obtained from
# https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database/data
#
# More datasets available here: https://archive.ics.uci.edu/
#
# The following variables were recorded:
#   * number of times pregnant
#   * plasma glucose concentration at 2 hours in an oral glucose tolerance test
#   * diastoling blood pressure (mmHg)
#   * triceps skin fold thickness (mm)
#   * 2-hour serum insulin (mu U/ml)
#   * body mass index (weight in kg/height in m^2)
#   * diabetes pedigree function ~ probability of diabetes based on family history
#   * age (years)
#   * test whether the patient showed signs of diabetes (coded zero if negative, one if positive)
# Read the CSV file
# Be sure to use the correct file path, you can check your current folder (working directory) with getwd()
data <- read.csv("./pima.csv")
##################################
# General info about the dataset #
# Print the structure of the data
str(data)
# Print the first few rows of the data
head(data)
#We can see that our dataset contains 768 lines and 9 variables
dim(data)
#We perform some numerical summaries:
summary(data)[,2] #choose just a variable
#Is it there something wrong with diastolic? No blood pressure is quite weird:
sort(data$diastolic)[1:50]
# Count the number of values equal to zero in the "diastolic" column
sum(data$diastolic == 0, na.rm = TRUE)
#We set all zero values of the five variables to NA which is the missing value code used by R:
data$diastolic[data$diastolic == 0] <- NA
data$glucose[data$glucose == 0] <- NA
data$triceps[data$triceps == 0] <- NA
data$insulin[data$insulin == 0] <- NA
data$bmi[data$bmi == 0] <- NA
# we have not set to NA all the zeros in data$test -> we don't have to!
# we have not set to NA all the zeros in data$pregnant -> we don't have to!
# Now the situation has changed:
summary(data)[,2]
# The variable test is not quantitative but categorical. It is convenient to designate them as factors to treat them appropriately:
data$test <- factor(data$test)
summary(data$test)   #We have 500 negative cases and 268 positive cases
# It is even better to use descriptive labels:
levels(data$test) <- c("negative", "positive")
summary(data)
###############
# Max and Min #
# Obtain the minimum pregnant times value
min(data$pregnant)
# Obtain the maximum pregnant times value
max(data$pregnant)
# Both can be obtained with the range command
extremes <- range(data$pregnant)
# Print the output
extremes
# Print the minimum
extremes[1]
# Print the maximum
extremes[2]
# Define a function to compute the actual range
rng <- function(x){
f0 <- max(x)-min(x)
return(f0)
}
# Apply the rng function to the pregnant variable
rng(data$pregnant)
# OR also:
# diff(range(data$pregnant))
########
# Mean #
# Compute with mean function for glucose level
mean(data$glucose,na.rm=TRUE)
#############
# Quartiles #
# Compute with the quantile function setting
# 0.25, 0.5 and 0.75 correspond to the first, second=median and third quartiles
quantile(data$glucose,na.rm=TRUE,0.25)
quantile(data$glucose,na.rm=TRUE,0.5)
quantile(data$glucose,na.rm=TRUE,0.75)
# Test any percentile
# 0.4 for 40% and 0.98 for 98%
quantile(data$glucose,na.rm=TRUE,0.4)
quantile(data$glucose,na.rm=TRUE,0.98)
# Interquartile range with IQR function or computing the difference manually
IQR(data$glucose,na.rm=TRUE)
quantile(data$glucose,na.rm=TRUE,0.75)-quantile(data$glucose,na.rm=TRUE,0.25)
###################################
# Standard deviation and variance #
# both are computed assuming we have a sample and not a population
# sd for standard deviation
sd(data$glucose,na.rm=TRUE)
# var for variance
var(data$glucose,na.rm=TRUE)
# apply the function to multiple columns with lapply
# standard deviation
lapply(data[,1:4],na.rm=TRUE,sd) # just for 4 variables
# variance
lapply(data[,1:4],na.rm=TRUE,var) # just for 4 variables
# in both cases we select all observations of data and four variables
########
# Mode #
# Let us create a procedure to compute the mode
# we first obtain the occurrences of each value
tab <- table(data$glucose)
# see the ouput
tab
# we then sort the output to obtain the value with the highest occurrence first
sort(tab,decreasing=TRUE)
# the same code may be applied to qualitative variables
sort(table(data$test),decreasing=TRUE)
############
# Packages #
# first we install them
# if the package is not installed simply uncomment the following line
# install.packages("pastecs")
# In general Rstudio will detect that we need to install a package
# we declare the package
library("pastecs")
# we use the function stat.desc which provides a summary of descriptive statistics
stat.desc(data)
# the output provides: number of variables, number of null, number of na, etc
# Always check the documentation.
# The docs for this particular function are in https://www.rdocumentation.org/packages/pastecs/versions/1.3.21/topics/stat.desc
###############
# Correlation #
cor(data$triceps, data$bmi)
?cor
# We can compute the correlation between two variables with the cor function
cor(data$triceps,data$bmi, use="complete.obs")
# the last argument is added to avoid NAs (complete.obs = complete observations)
# The result is the Pearson correlation
#####################
# Contingency Table #
# we first use create a new variable from the values in age
# we will define the values "young" and "old" for lengths that are smaller or larger than the median
data$age <- ifelse(data$age < median(data$age),"young","old")
# let us visualize the variable
table(data$age)
# we now create a contingency table between our two qualitative variables
table(data$age,data$test)
# a similar output is produced with xtabs
xtabs(~data$age + data$test)
# let us now compute the proportions (with respect to all)
prop.table(table(data$age,data$test))
# proportions can be computed by row (1) or by column (2)
prop.table(table(data$age,data$test), 1) # (proportions computed through columns)
prop.table(table(data$age,data$test), 2) # (proportions computed through rows)
?prop.table
# we can round the results to 2 digits
round(prop.table(table(data$age,data$test),1),2)
round(prop.table(table(data$age,data$test),2),2)
### PLOTTING ###
###############
# Mosaic Plot #
mosaicplot(table(data$age,data$test),
color = TRUE,
xlab= "Age",
ylab= "Test"
)
# a mosaic plot can also be produced with the package vcd
library(vcd)
mosaic(~ age + test,
data=data,
direction=c("v","h")
)
############
# Bar plot #
levels(data$test)
table(data$test)
# bar plots for qualitative variables
barplot(table(data$test))
# one can also represent the relative frequencies
barplot(prop.table(table(data$test)))
# barplots can also be produced with the ggplot2 package
library(ggplot2)
ggplot(data)+
aes(test)+
geom_bar()
#############
# Histogram #
#The most well-known univariate plot is the histogram:
hist(data$diastolic,
xlab = "Diastolic",
main = "Title here")
# use breaks if you want to customize the number of bins
# histograms are also produced with ggplot2
ggplot(data)+
aes(x=diastolic)+
geom_histogram()
# the number of bins is 30 by default, change it with bins
ggplot(data)+
aes(x=diastolic)+
geom_histogram(bins=10)
###########
# Boxplot #
# the boxplot provides q1,q2,q3 and the minimum and maximum values
boxplot(data$diastolic, na.rm=TRUE)
# An outlier is that observation that is very distant from the rest of the data. A data point is said to be an outlier if it is greater than Q3+1.5IQR or lower than Q1-1.5 IQR
# mean(data$diastolic, na.rm=TRUE)
# median(data$diastolic, na.rm=TRUE)
# quantile(data$diastolic, na.rm=TRUE,0.75)
# max(data$diastolic, na.rm=TRUE)
# we can combine the information of different variables
boxplot(data$diastolic ~ data$age, na.rm=TRUE)
# are there outliers?
# boxplots in ggpot2
ggplot(data)+
aes(x=age,y=diastolic)+
geom_boxplot()
###########
# Dot plot #
library(lattice)
dotplot(data$diastolic ~ data$age)
# it's very easy to see the outlier in this plot
# in ggplot2 we have
ggplot(data)+
aes(x=age,y=diastolic)+
geom_dotplot(binaxis="y",stackdir="center")
# the dot plot in ggplot2 provides more information than the boxplot or lattice, but it' only practical with few observations
###########
# Violin plot #
# check this out:
# https://seaborn.pydata.org/generated/seaborn.violinplot.html
###############
# Scatter plot #
# scatter plots can provide the first hint of a relation between variables
plot(data$diastolic,data$diabetes)
cor(data$diastolic,data$diabetes,
use = "complete.obs")
plot(data$insulin,data$glucose)
cor(data$insulin,data$glucose,
use = "complete.obs")
# in ggplot2
ggplot(data)+
aes(x=diastolic,y=diabetes)+
geom_point()
# we can divide the observation in groups (e.g., age groups)
ggplot(data)+
aes(x=diastolic,y=diabetes,colour=age)+
geom_point()+
scale_color_hue()
#############
# Line plot #
plot(data$diastolic,type="l")
# the index is not important here, let's sort the lengths
plot(sort(data$diastolic),type="l")
# a qq plot allows to check the normality assumption
qqnorm(data$diastolic)
qqline(data$diastolic)
# the car package allows us to provide confidence bounds
# install.packages("car")
library(car)
qqPlot(data$diastolic)
# the same plot can be done with the ggpubr library
library(ggpubr)
ggqqplot(data$diastolic)
# we may be interested in checking the normality in groups
qqPlot(data$diastolic,groups=data$age)
################
# Density plot #
plot(density(data$diastolic,na.rm=TRUE),main="") #na.rm is an argument that is commonly used in functions to specify whether to remove NA values before performing a calculation.
# and in ggplot2
ggplot(data)+
aes(x=diastolic)+
geom_density()
###########
# sorted data against its index #
plot(sort(data$diastolic), ylab="Sorted diastolic")
###########
# bivariate plots #
plot(diabetes ~ diastolic, data) #Scatter plot showing two quantitative variables
plot(diabetes ~ test, data)  #Side-by-side boxplot showing a qualitative and a quatitative variable
### USE OF ggplot2 ###
require(ggplot2)
ggplot(data, aes(x=diastolic)) + geom_histogram()    #aes (Aesthetic mappings)
ggplot(data, aes(x=diastolic)) + geom_density()
ggplot(data, aes(x=diastolic, y=diabetes)) + geom_point()
#Extra versatility:
ggplot(data, aes(x=diastolic, y=diabetes, shape=test)) + geom_point() +
theme(legend.position = "top", legend.direction = "horizontal")
#or
ggplot(data, aes(x=diastolic, y=diabetes)) + geom_point(size=1) + facet_grid(.~test)
# Let us explore relationships among all numerical variables:
# Example: Subset dataframe to include only numeric variables
numeric_df <- data[, sapply(data, is.numeric)]
# Create multiscatter plot
pairs(numeric_df)
#We can see that, for example, there seems to be a positive correlation between bmi and triceps:
ggplot(data, aes(x=bmi, y=triceps)) + geom_point()
cor(data$triceps, data$bmi, use = "complete.obs")
setwd("~/UNI/2_SEMESTRE/ADVANCED STATISTICS/PROJECT/cancerResearch")
# DESCRIPTIVE ANALYSIS OF THE WISCONSIN BREAST CANCER DATASET
# Authors: Iker Ibarrola, Enetz Quindimil, Carlos Firvida, IÃ±igo Infante
# Description of the dataset:
# First of all, we load the libraries we are going to use to perform the analysis
library(dplyr)
library(pastecs)
library(ggplot2)
library(lattice)
library(car)
###################################
# START
# We read the dataset from the csv file
data <- read.csv("./data.csv")
# We get some general information from the dataset, ensuring that everything has been loaded correctly
head(data) # We visualize the first six rows of the dataset
str(data) # We visualize the structure of the data (the different variables, their type and some initial values)
# After this two first steps, we detect that there is a variable called id, which will not be useful for us
data <- data %>% select(-id) # We remove id
# We check the dimensions of the dataset (569 rows and 31 columns)
dim(data)
# Now we get the numerical summary for some variable(s)
summary(data) #[,] # We can add the [] to select specific variables
# After performing this analysis, we detected that the minimum value of some variables is 0 (the ones related to concavity).
# As we are not experts in this field (medicine, precisely breast cancer), at first we could not tell
# whether this observations could be correct or maybe something had gone wrong during the measurements.
# We decided to do some research and it seems that this is uncommon, yet possible (especially in benign tumors).
# This finding might help us later in order to find correlations and patterns
#As we know that is unlikely, we check which is the proportion of the observations that have concavity_mean = 0
sum(data$concavity_mean == 0, na.rm = TRUE) / dim(data)[1] # Around 0.02 (2%) which could make sense
# The variable diagnosis is categorical (binary). So we will create a factor from it:
data$diagnosis <- factor(data$diagnosis)
# We check that know the data is interpreted as a factor rather than a simple char
summary(data$diagnosis)   # There are have 357  benign tumor cases and 212  malign tumour cases
# Instead of using just 'b' and 'm', we change to more descriptive labels:
levels(data$diagnosis) <- c("benign", "malign")
# We check again that this change has been executed appropriately
summary(data$diagnosis)
# We are going to check as well for the existence of na observations (missing values)
colSums(is.na(data)) # There are no missing values
# We could use this function as well to check for some descriptive statistics and missing values
stat.desc(data)
###################################
# Metrics such as minimum, maximum, median, mean and 1st and 3rd quartiles are already computed with the summary() function
# However, if at any moment we would like to compute each of them separately, we could do it in the following way:
# *we have omitted the na.rm=TRUE parameter as we have previously checked that there are no missing values
variable_to_analyse <- data$radius_mean
min(variable_to_analyse)
max(variable_to_analyse)
median(variable_to_analyse)
mean(variable_to_analyse)
quantile(variable_to_analyse, 0.25) # First quartile
quantile(variable_to_analyse, 0.75) # Third quartile
# Additionally, we could compute some other metrics (we haven't include the mode as the variables are continuous)
range(variable_to_analyse) # Range of the variable
quantile(variable_to_analyse, 0.4) # Any percentile of the variable
IQR(variable_to_analyse) # Interquartile range (this would be later visualize via boxplots)
sd(variable_to_analyse) # Standard deviation
var(variable_to_analyse) # Variance
# In case we would want to use one of this functions over a subset of variables:
subset_to_analyse <- data[, 1:31] # Select the subset you want
lapply(subset_to_analyse, mean) # Modify the second parameter for whatever function you want
###################################
# Correlation
# In this step we will do a deeper analysis of our variables: we already knew that there were "just" 10 different metrics
# (radius, texture, perimeter, area, smoothness, compactness, concavity, concave points, symmetry and fractal dimension),
# each computed in three different ways (mean, se (standard error) and worst).
# Now, we will check if there is any correlation between each metric's mean and se.
cor(data$radius_mean, data$radius_se)
cor(data$texture_mean, data$texture_se)
cor(data$perimeter_mean, data$perimeter_se)
cor(data$area_mean, data$area_se)
cor(data$smoothness_mean, data$smoothness_se)
cor(data$compactness_mean, data$compactness_se)
cor(data$concavity_mean, data$concavity_se)
cor(data$concave.points_mean, data$concave.points_se)
cor(data$symmetry_mean, data$symmetry_se)
cor(data$fractal_dimension_mean, data$fractal_dimension_se)
# In most cases the correlation has been between 0.6 and 0.7 (a positive correlation but not very strong)
# In other cases such as the smoothness, texture symmetry the correlation has been weaker (around 0.3-0.4)
# Finally, in the case of area the correlation has been strong (0.8)
# We are going to check the correlation between each metric's mean and worst (in this case we can already suspect that
#the correlation is going to be strong, as higher means should have a higher worst value)
cor(data$radius_mean, data$radius_worst)
cor(data$texture_mean, data$texture_worst)
cor(data$perimeter_mean, data$perimeter_worst)
cor(data$area_mean, data$area_worst)
cor(data$smoothness_mean, data$smoothness_worst)
cor(data$compactness_mean, data$compactness_worst)
cor(data$concavity_mean, data$concavity_worst)
cor(data$concave.points_mean, data$concave.points_worst)
cor(data$symmetry_mean, data$symmetry_worst)
cor(data$fractal_dimension_mean, data$fractal_dimension_worst)
# As suspected, in most cases the correlation was very strong (>0.85). In the cases of symmetry, smoothness and
# fractal_dimension the correlation was lower but still positive and considerably strong.
# Finally we compute the correlation between standard errors and worst, obtaining similar results to the ones
# comparing mean and se
cor(data$radius_se, data$radius_worst)
cor(data$texture_se, data$texture_worst)
cor(data$perimeter_se, data$perimeter_worst)
cor(data$area_se, data$area_worst)
cor(data$smoothness_se, data$smoothness_worst)
cor(data$compactness_se, data$compactness_worst)
cor(data$concavity_se, data$concavity_worst)
cor(data$concave.points_se, data$concave.points_worst)
cor(data$symmetry_se, data$symmetry_worst)
cor(data$fractal_dimension_se, data$fractal_dimension_worst)
# We check the correlation of some variables
cor(data$radius_mean, data$texture_mean) # Low correlation (0.32)
cor(data$radius_mean, data$smoothness_mean) # Very low correlation (0.17)
cor(data$radius_mean, data$concave.points_mean) # Strong correlation (0.82)
# For instance, we know that the correlation of radius & perimeter, radius & area, and perimeter & area are going
# to be extremely strong, as there is a linear relationship between them (thus, we couldn't use both for a linear model)
cor(data$radius_mean, data$perimeter_mean) # Very strong correlation (over 0.99, almost perfect correlation)
cor(data$radius_mean, data$area_mean) # Very strong correlation (over 0.98, near perfect correlation)
cor(data$perimeter_mean, data$area_mean) # Very strong correlation (over 0.98, near perfect correlation)
###################################
# Visualization
# To begin with the visualization, we will create the multiscatter plot for the variables computing
# the mean, the se and worst:
data_means <- data[,2:11]
numeric_df_means <- data_means[, sapply(data_means, is.numeric)]
pairs(numeric_df_means) # for the means
data_se <- data[,12:21]
numeric_df_se <- data_se[, sapply(data_se, is.numeric)]
pairs(numeric_df_se) # for the standard error
data_worst <- data[,22:31]
numeric_df_worst <- data_worst[, sapply(data_worst, is.numeric)]
pairs(numeric_df_worst) # for the standard error
# We continue by creating a histogram of radius_mean (in case you wanted to visualize
# another variable just change x = variable)
ggplot(data, aes(x = radius_mean)) +
geom_histogram(binwidth = 0.5, fill = "blue") +
ggtitle("Histogram of Radius Mean") +
xlab("Radius Mean") +
ylab("Count")
# Now a boxplot of radius_mean (in case you wanted to visualize another variable just change x = variable)
# we use the diagnosis in the x axis to see difference between belign and malign tumors
ggplot(data, aes(x = diagnosis , y = radius_mean, fill = diagnosis)) +
geom_boxplot() +
ggtitle("Comparison of Radius Mean for Benign vs Malignant Tumors") # We can detect some outliers by using it
# To confirm them we use the dotplot
dotplot(data$radius_mean ~ data$diagnosis)
# We use some other charts such as the violin plot for concavity_mean
ggplot(data, aes(x = diagnosis, y = concavity_mean, fill = diagnosis)) +
geom_violin() +
ggtitle("Distribution of Concavity Mean by Diagnosis")
# Scatter plot of radius_mean vs perimeter_mean
ggplot(data, aes(x = radius_mean, y = concavity_mean, color = diagnosis)) +
geom_point() +
ggtitle("Scatter Plot of Radius Mean vs Perimeter Mean")
# Scatter plot of smoothness_mean vs compactness_mean
ggplot(data, aes(x = smoothness_mean, y = compactness_mean, color = diagnosis)) +
geom_point() +
ggtitle("Scatter Plot of Radius Mean vs Perimeter Mean")
# We continue by using the contingency table to check some information about benign and malign tumors
table(data$diagnosis)
# We look at the proportion of benign vs malignant cases
prop.table(table(data$diagnosis))
# We use a bar plot to visualize this
ggplot(data)+
aes(diagnosis)+
geom_bar()
# We end by using some other charts/plots
# qq plot
# We end by using some other charts/plots
# qq plot
qqPlot(data$radius_mean)
# Scatter plot of smoothness_mean vs compactness_mean
ggplot(data, aes(x = smoothness_mean, y = compactness_mean, color = diagnosis)) +
geom_point() +
ggtitle("Scatter Plot of Radius Mean vs Perimeter Mean")
#Another way of visualizing the same as above
ggplot(data, aes(x=smoothness_mean, y=compactness_mean)) + geom_point(size=1) + facet_grid(.~diagnosis)
#Another way of visualizing the same as above
ggplot(data, aes(x=radius_mean, y=concavity_mean)) + geom_point(size=1) + facet_grid(.~diagnosis)
# We end by using the qq plot
qqPlot(data$radius_mean) # as there are a lot of points outside the region,the normal approximation does not work
# We check the correlation of some variables
cor(data$radius_mean, data$texture_mean) # Low correlation (0.32)
###################################
# Visualization
# To begin with the visualization, we will create the multiscatter plot for the variables computing
# the mean, the se and worst:
data_means <- data[,2:11]
numeric_df_means <- data_means[, sapply(data_means, is.numeric)]
pairs(numeric_df_means) # for the means
pairs(numeric_df_means) # for the means
data_se <- data[,12:21]
numeric_df_se <- data_se[, sapply(data_se, is.numeric)]
pairs(numeric_df_se) # for the standard error
data_worst <- data[,22:31]
numeric_df_worst <- data_worst[, sapply(data_worst, is.numeric)]
pairs(numeric_df_worst) # for the standard error
